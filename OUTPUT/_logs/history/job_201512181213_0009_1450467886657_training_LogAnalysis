Meta VERSION="1" .
Job JOBID="job_201512181213_0009" JOBNAME="LogAnalysis" USER="training" SUBMIT_TIME="1450467886657" JOBCONF="hdfs://0\.0\.0\.0:8020/var/lib/hadoop-hdfs/cache/mapred/mapred/staging/training/\.staging/job_201512181213_0009/job\.xml" VIEW_JOB="*" MODIFY_JOB="*" JOB_QUEUE="default" .
Job JOBID="job_201512181213_0009" JOB_PRIORITY="NORMAL" .
Job JOBID="job_201512181213_0009" LAUNCH_TIME="1450467887273" TOTAL_MAPS="25" TOTAL_REDUCES="1" JOB_STATUS="PREP" .
Task TASKID="task_201512181213_0009_m_000026" TASK_TYPE="SETUP" START_TIME="1450467887434" SPLITS="" .
MapAttempt TASK_TYPE="SETUP" TASKID="task_201512181213_0009_m_000026" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000026_0" START_TIME="1450467887552" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="SETUP" TASKID="task_201512181213_0009_m_000026" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000026_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467890043" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="setup" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(191670)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(0)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(1)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(70)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(81088512)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(720445440)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(31850496)]}nullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000026" TASK_TYPE="SETUP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467890151" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(191670)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(0)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(1)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(70)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(81088512)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(720445440)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(31850496)]}nullnullnullnullnullnullnullnullnullnullnullnullnull" .
Job JOBID="job_201512181213_0009" JOB_STATUS="RUNNING" .
Task TASKID="task_201512181213_0009_m_000000" TASK_TYPE="MAP" START_TIME="1450467890158" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000001" TASK_TYPE="MAP" START_TIME="1450467890160" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000000" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000000_0" START_TIME="1450467890168" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000000" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000000_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467896671" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log0\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237221)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43514)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(190824448)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000000" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467896796" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237221)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43514)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(190824448)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000001" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000001_0" START_TIME="1450467890169" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000001" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000001_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467896660" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log1\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237181)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43474)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(700)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(189358080)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000001" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467896798" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237181)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43474)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(700)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(189358080)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000002" TASK_TYPE="MAP" START_TIME="1450467896799" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000003" TASK_TYPE="MAP" START_TIME="1450467896800" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_r_000000" TASK_TYPE="REDUCE" START_TIME="1450467896800" SPLITS="" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000002" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000002_0" START_TIME="1450467896806" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000002" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000002_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467905949" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log10\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238183)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44476)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(191344640)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000002" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467906150" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238183)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44476)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(191344640)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000003" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000003_0" START_TIME="1450467896807" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000003" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000003_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467905939" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log11\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238188)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44481)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(185720832)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000003" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467906153" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238188)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44481)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(185720832)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000004" TASK_TYPE="MAP" START_TIME="1450467906154" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000005" TASK_TYPE="MAP" START_TIME="1450467906158" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000004" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000004_0" START_TIME="1450467906162" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000004" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000004_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467912948" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log12\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238204)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44497)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(189587456)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000004" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467913091" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238204)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44497)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(189587456)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000005" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000005_0" START_TIME="1450467906167" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000005" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000005_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467912898" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log13\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238229)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44522)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(191229952)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000005" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467913092" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238229)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44522)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(191229952)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000006" TASK_TYPE="MAP" START_TIME="1450467913092" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000007" TASK_TYPE="MAP" START_TIME="1450467913093" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000006" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000006_0" START_TIME="1450467913094" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000006" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000006_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467919756" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log14\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238208)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44501)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(190746624)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000006" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467920034" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238208)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44501)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(190746624)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000007" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000007_0" START_TIME="1450467913094" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000007" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000007_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467919777" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log15\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238184)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44477)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(191832064)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000007" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467920036" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238184)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44477)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(191832064)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000008" TASK_TYPE="MAP" START_TIME="1450467920038" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000009" TASK_TYPE="MAP" START_TIME="1450467920038" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000008" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000008_0" START_TIME="1450467920041" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000008" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000008_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467926541" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log16\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238167)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44460)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(680)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(190894080)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000008" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467926670" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238167)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44460)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(680)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(190894080)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000009" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000009_0" START_TIME="1450467920042" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000009" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000009_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467926621" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log17\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238178)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44471)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(750)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(186384384)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000009" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467926671" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238178)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44471)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(750)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(186384384)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000010" TASK_TYPE="MAP" START_TIME="1450467926672" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000011" TASK_TYPE="MAP" START_TIME="1450467926672" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000010" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000010_0" START_TIME="1450467926673" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000010" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000010_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467933226" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log18\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238196)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44488)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(700)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(186351616)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000010" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467933306" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238196)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44488)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(700)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(186351616)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000011" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000011_0" START_TIME="1450467926674" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000011" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000011_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467933235" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log19\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238118)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44410)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(189587456)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000011" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467933307" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238118)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44410)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(189587456)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000012" TASK_TYPE="MAP" START_TIME="1450467933308" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000013" TASK_TYPE="MAP" START_TIME="1450467933310" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000012" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000012_0" START_TIME="1450467933315" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000012" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000012_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467939826" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log2\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237193)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43485)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(680)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(188968960)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000012" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467939948" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237193)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43485)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(680)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(188968960)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000013" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000013_0" START_TIME="1450467933318" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000013" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000013_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467939839" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log20\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238184)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44476)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(190783488)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000013" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467939951" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238184)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44476)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(190783488)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000014" TASK_TYPE="MAP" START_TIME="1450467939952" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000015" TASK_TYPE="MAP" START_TIME="1450467939953" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000014" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000014_0" START_TIME="1450467939955" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000014" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000014_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467946472" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log21\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238146)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44438)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(680)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(188186624)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000014" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467946582" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238146)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44438)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(680)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(188186624)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000015" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000015_0" START_TIME="1450467939956" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000015" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000015_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467946463" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log22\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238164)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44456)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(191160320)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000015" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467946584" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238164)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44456)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(191160320)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000016" TASK_TYPE="MAP" START_TIME="1450467946587" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000017" TASK_TYPE="MAP" START_TIME="1450467946588" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000016" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000016_0" START_TIME="1450467946596" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000016" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000016_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467953300" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log23\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238172)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44464)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(720)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(187707392)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000016" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467953530" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238172)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44464)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(720)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(187707392)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000017" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000017_0" START_TIME="1450467946599" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000017" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000017_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467953334" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log24\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238975)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(45267)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(191729664)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000017" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467953533" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238975)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99117)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(45267)][(SPLIT_RAW_BYTES)(Input split bytes)(117)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(710)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(191729664)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000018" TASK_TYPE="MAP" START_TIME="1450467953534" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000019" TASK_TYPE="MAP" START_TIME="1450467953535" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000018" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000018_0" START_TIME="1450467953538" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000018" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000018_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467960121" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log3\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237134)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43426)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(620)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(186548224)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000018" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467960203" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237134)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43426)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(620)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(186548224)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000019" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000019_0" START_TIME="1450467953539" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000019" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000019_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467960191" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log4\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237223)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43515)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(188530688)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000019" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467960204" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237223)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43515)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(188530688)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000020" TASK_TYPE="MAP" START_TIME="1450467960205" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000021" TASK_TYPE="MAP" START_TIME="1450467960205" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000020" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000020_0" START_TIME="1450467960210" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000020" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000020_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467966511" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log5\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237183)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43475)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(720)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(187359232)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000020" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467966617" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237183)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43475)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(720)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(187359232)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000021" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000021_0" START_TIME="1450467960219" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000021" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000021_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467966496" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log6\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237155)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43447)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(190517248)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000021" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467966619" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237155)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43447)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(190517248)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000022" TASK_TYPE="MAP" START_TIME="1450467966630" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512181213_0009_m_000023" TASK_TYPE="MAP" START_TIME="1450467966630" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000022" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000022_0" START_TIME="1450467966633" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000022" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000022_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467973101" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log7\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237152)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43444)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(700)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(189865984)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000022" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467973297" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(237152)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(43444)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(700)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(189865984)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000023" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000023_0" START_TIME="1450467966633" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000023" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000023_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467973093" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log8\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238173)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44465)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(720)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(188370944)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000023" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467973300" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238173)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44465)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(720)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(188370944)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000024" TASK_TYPE="MAP" START_TIME="1450467973301" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000024" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000024_0" START_TIME="1450467973304" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512181213_0009_m_000024" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000024_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467976367" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputDataTask3/Log9\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238188)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44480)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(630)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(187572224)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000024" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467976621" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(238188)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99116)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(44480)][(SPLIT_RAW_BYTES)(Input split bytes)(116)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(630)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(187572224)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(131534848)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
ReduceAttempt TASK_TYPE="REDUCE" TASKID="task_201512181213_0009_r_000000" TASK_ATTEMPT_ID="attempt_201512181213_0009_r_000000_0" START_TIME="1450467896810" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
ReduceAttempt TASK_TYPE="REDUCE" TASKID="task_201512181213_0009_r_000000" TASK_ATTEMPT_ID="attempt_201512181213_0009_r_000000_0" TASK_STATUS="SUCCESS" SHUFFLE_FINISHED="1450467977326" SORT_FINISHED="1450467978798" FINISH_TIME="1450467980318" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="reduce > reduce" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(1154615)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(1345892)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(9858)][(HDFS_READ_OPS)(HDFS: Number of read operations)(6)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(3)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_INPUT_GROUPS)(Reduce input groups)(275)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(1154759)][(REDUCE_INPUT_RECORDS)(Reduce input records)(25000)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(275)][(SPILLED_RECORDS)(Spilled Records)(25000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(2880)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(102678528)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(728879104)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(31850496)]}{(solution\.Reduce$Counters)(solution\.Reduce$Counters)[(OUTPUT_KEYS)(OUTPUT_KEYS)(275)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_r_000000" TASK_TYPE="REDUCE" TASK_STATUS="SUCCESS" FINISH_TIME="1450467980547" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(1154615)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(1345892)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(9858)][(HDFS_READ_OPS)(HDFS: Number of read operations)(6)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(3)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_INPUT_GROUPS)(Reduce input groups)(275)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(1154759)][(REDUCE_INPUT_RECORDS)(Reduce input records)(25000)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(275)][(SPILLED_RECORDS)(Spilled Records)(25000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(2880)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(102678528)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(728879104)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(31850496)]}{(solution\.Reduce$Counters)(solution\.Reduce$Counters)[(OUTPUT_KEYS)(OUTPUT_KEYS)(275)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000025" TASK_TYPE="CLEANUP" START_TIME="1450467980549" SPLITS="" .
MapAttempt TASK_TYPE="CLEANUP" TASKID="task_201512181213_0009_m_000025" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000025_0" START_TIME="1450467980552" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:44739" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="CLEANUP" TASKID="task_201512181213_0009_m_000025" TASK_ATTEMPT_ID="attempt_201512181213_0009_m_000025_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450467983098" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="cleanup" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(191670)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(2)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(90)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(82030592)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(31850496)]}nullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512181213_0009_m_000025" TASK_TYPE="CLEANUP" TASK_STATUS="SUCCESS" FINISH_TIME="1450467983262" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(191670)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(2)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(90)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(82030592)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(31850496)]}nullnullnullnullnullnullnullnullnullnullnullnullnull" .
Job JOBID="job_201512181213_0009" FINISH_TIME="1450467983266" JOB_STATUS="SUCCESS" FINISHED_MAPS="25" FINISHED_REDUCES="1" FAILED_MAPS="0" FAILED_REDUCES="0" MAP_COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(5947299)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(2477915)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(50)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(25000)][(MAP_OUTPUT_RECORDS)(Map output records)(25000)][(MAP_OUTPUT_BYTES)(Map output bytes)(1104609)][(SPLIT_RAW_BYTES)(Input split bytes)(2915)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(25000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(17430)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(4731162624)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(18037452800)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(3288371200)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(2475000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" REDUCE_COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(1154615)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(1345892)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(9858)][(HDFS_READ_OPS)(HDFS: Number of read operations)(6)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(3)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_INPUT_GROUPS)(Reduce input groups)(275)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(1154759)][(REDUCE_INPUT_RECORDS)(Reduce input records)(25000)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(275)][(SPILLED_RECORDS)(Spilled Records)(25000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(2880)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(102678528)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(728879104)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(31850496)]}{(solution\.Reduce$Counters)(solution\.Reduce$Counters)[(OUTPUT_KEYS)(OUTPUT_KEYS)(275)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(1154615)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(7293191)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(2477915)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(9858)][(HDFS_READ_OPS)(HDFS: Number of read operations)(56)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(3)]}{(org\.apache\.hadoop\.mapreduce\.JobCounter)(Job Counters )[(TOTAL_LAUNCHED_MAPS)(Launched map tasks)(25)][(TOTAL_LAUNCHED_REDUCES)(Launched reduce tasks)(1)][(DATA_LOCAL_MAPS)(Data-local map tasks)(25)][(SLOTS_MILLIS_MAPS)(Total time spent by all maps in occupied slots \\(ms\\))(170660)][(SLOTS_MILLIS_REDUCES)(Total time spent by all reduces in occupied slots \\(ms\\))(83508)][(FALLOW_SLOTS_MILLIS_MAPS)(Total time spent by all maps waiting after reserving slots \\(ms\\))(0)][(FALLOW_SLOTS_MILLIS_REDUCES)(Total time spent by all reduces waiting after reserving slots \\(ms\\))(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(25000)][(MAP_OUTPUT_RECORDS)(Map output records)(25000)][(MAP_OUTPUT_BYTES)(Map output bytes)(1104609)][(SPLIT_RAW_BYTES)(Input split bytes)(2915)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_INPUT_GROUPS)(Reduce input groups)(275)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(1154759)][(REDUCE_INPUT_RECORDS)(Reduce input records)(25000)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(275)][(SPILLED_RECORDS)(Spilled Records)(50000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(20310)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(4833841152)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(18766331904)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(3320221696)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(2475000)]}{(solution\.Reduce$Counters)(solution\.Reduce$Counters)[(OUTPUT_KEYS)(OUTPUT_KEYS)(275)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
