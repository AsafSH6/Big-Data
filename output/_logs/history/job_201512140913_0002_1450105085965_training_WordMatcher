Meta VERSION="1" .
Job JOBID="job_201512140913_0002" JOBNAME="WordMatcher" USER="training" SUBMIT_TIME="1450105085965" JOBCONF="hdfs://0\.0\.0\.0:8020/var/lib/hadoop-hdfs/cache/mapred/mapred/staging/training/\.staging/job_201512140913_0002/job\.xml" VIEW_JOB="*" MODIFY_JOB="*" JOB_QUEUE="default" .
Job JOBID="job_201512140913_0002" JOB_PRIORITY="NORMAL" .
Job JOBID="job_201512140913_0002" LAUNCH_TIME="1450105086139" TOTAL_MAPS="25" TOTAL_REDUCES="1" JOB_STATUS="PREP" .
Task TASKID="task_201512140913_0002_m_000026" TASK_TYPE="SETUP" START_TIME="1450105086411" SPLITS="" .
MapAttempt TASK_TYPE="SETUP" TASKID="task_201512140913_0002_m_000026" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000026_0" START_TIME="1450105086559" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="SETUP" TASKID="task_201512140913_0002_m_000026" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000026_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105089197" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="setup" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(191675)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(0)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(1)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(70)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(92958720)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(720445440)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(56033280)]}nullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000026" TASK_TYPE="SETUP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105089438" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(191675)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(0)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(1)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(70)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(92958720)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(720445440)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(56033280)]}nullnullnullnullnullnullnullnullnullnullnullnullnull" .
Job JOBID="job_201512140913_0002" JOB_STATUS="RUNNING" .
Task TASKID="task_201512140913_0002_m_000000" TASK_TYPE="MAP" START_TIME="1450105089443" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512140913_0002_m_000001" TASK_TYPE="MAP" START_TIME="1450105089444" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000000" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000000_0" START_TIME="1450105089453" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000000" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000000_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105095914" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log0\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259004)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65292)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(890)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(204238848)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000000" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105096093" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259004)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65292)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(890)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(204238848)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000001" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000001_0" START_TIME="1450105089455" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000001" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000001_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105095998" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log1\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(258830)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65118)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(890)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202649600)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000001" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105096098" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(258830)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65118)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(890)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202649600)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000002" TASK_TYPE="MAP" START_TIME="1450105096101" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512140913_0002_m_000003" TASK_TYPE="MAP" START_TIME="1450105096102" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512140913_0002_r_000000" TASK_TYPE="REDUCE" START_TIME="1450105096103" SPLITS="" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000002" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000002_0" START_TIME="1450105096108" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000002" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000002_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105105181" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log10\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260110)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66398)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(660)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(200445952)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000002" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105105474" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260110)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66398)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(660)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(200445952)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000004" TASK_TYPE="MAP" START_TIME="1450105105476" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000003" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000003_0" START_TIME="1450105096119" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000003" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000003_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105105666" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log11\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260107)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66395)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(750)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202440704)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000003" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105105780" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260107)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66395)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(750)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202440704)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000005" TASK_TYPE="MAP" START_TIME="1450105105781" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000004" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000004_0" START_TIME="1450105105482" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000004" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000004_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105111281" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log12\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260566)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66854)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(650)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201850880)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000004" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105111518" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260566)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66854)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(650)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201850880)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000006" TASK_TYPE="MAP" START_TIME="1450105111519" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000005" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000005_0" START_TIME="1450105105787" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000005" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000005_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105111625" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log13\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260390)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66678)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(610)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(200986624)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000005" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105111822" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260390)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66678)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(610)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(200986624)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000007" TASK_TYPE="MAP" START_TIME="1450105111824" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000006" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000006_0" START_TIME="1450105111525" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000006" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000006_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105117462" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log14\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260043)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66331)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(620)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(197263360)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000006" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105117596" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260043)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66331)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(620)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(197263360)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000008" TASK_TYPE="MAP" START_TIME="1450105117597" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000007" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000007_0" START_TIME="1450105111828" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000007" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000007_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105117822" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log15\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260009)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66297)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(640)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201015296)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000007" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105117899" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260009)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66297)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(640)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201015296)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000009" TASK_TYPE="MAP" START_TIME="1450105117901" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000008" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000008_0" START_TIME="1450105117605" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000008" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000008_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105123684" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log16\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259956)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66244)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(640)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(203341824)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000008" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105123945" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259956)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66244)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(640)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(203341824)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000009" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000009_0" START_TIME="1450105117907" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000009" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000009_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105123847" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log17\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259936)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66224)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(630)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201068544)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000009" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105123953" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259936)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66224)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(630)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201068544)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000010" TASK_TYPE="MAP" START_TIME="1450105123958" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512140913_0002_m_000011" TASK_TYPE="MAP" START_TIME="1450105123959" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000010" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000010_0" START_TIME="1450105123965" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000010" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000010_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105130131" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log18\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259895)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66182)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(630)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(198905856)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000010" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105130311" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259895)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66182)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(630)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(198905856)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000011" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000011_0" START_TIME="1450105123967" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000011" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000011_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105130111" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log19\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259927)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66214)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(630)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201678848)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000011" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105130316" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259927)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66214)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(630)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201678848)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000012" TASK_TYPE="MAP" START_TIME="1450105130321" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512140913_0002_m_000013" TASK_TYPE="MAP" START_TIME="1450105130322" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000012" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000012_0" START_TIME="1450105130329" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000012" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000012_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105136673" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log2\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259172)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65459)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(650)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(203350016)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000012" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105136686" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259172)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65459)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(650)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(203350016)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000013" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000013_0" START_TIME="1450105130330" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000013" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000013_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105136617" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log20\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260078)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66365)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(660)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(200871936)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000013" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105136688" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260078)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66365)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(660)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(200871936)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000014" TASK_TYPE="MAP" START_TIME="1450105136690" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512140913_0002_m_000015" TASK_TYPE="MAP" START_TIME="1450105136690" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000014" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000014_0" START_TIME="1450105136692" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000014" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000014_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105142892" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log21\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259684)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65971)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(630)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201560064)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000014" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105143026" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259684)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65971)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(630)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201560064)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000015" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000015_0" START_TIME="1450105136692" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000015" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000015_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105142921" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log22\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260040)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66327)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(660)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(203026432)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000015" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105143027" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260040)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66327)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(660)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(203026432)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000016" TASK_TYPE="MAP" START_TIME="1450105143029" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512140913_0002_m_000017" TASK_TYPE="MAP" START_TIME="1450105143030" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000016" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000016_0" START_TIME="1450105143034" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000016" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000016_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105149387" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log23\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260289)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66576)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(203493376)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000016" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105149696" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260289)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66576)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(690)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(203493376)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000017" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000017_0" START_TIME="1450105143036" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000017" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000017_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105149471" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log24\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260785)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(67072)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(740)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202481664)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000017" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105149698" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260785)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99115)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(67072)][(SPLIT_RAW_BYTES)(Input split bytes)(115)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(740)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202481664)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000018" TASK_TYPE="MAP" START_TIME="1450105149702" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512140913_0002_m_000019" TASK_TYPE="MAP" START_TIME="1450105149703" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000018" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000018_0" START_TIME="1450105149705" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000018" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000018_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105156378" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log3\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(258780)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65067)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(650)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202870784)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000018" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105156393" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(258780)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65067)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(650)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202870784)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000019" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000019_0" START_TIME="1450105149705" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000019" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000019_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105156387" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log4\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259092)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65379)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(650)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(204046336)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000019" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105156395" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(259092)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65379)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(650)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(204046336)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000020" TASK_TYPE="MAP" START_TIME="1450105156396" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512140913_0002_m_000021" TASK_TYPE="MAP" START_TIME="1450105156396" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000020" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000020_0" START_TIME="1450105156402" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000020" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000020_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105162622" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log5\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(258910)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65197)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(640)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202960896)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000020" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105162738" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(258910)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65197)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(640)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202960896)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000021" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000021_0" START_TIME="1450105156403" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000021" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000021_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105162629" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log6\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(258897)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65184)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(650)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(200892416)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000021" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105162742" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(258897)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65184)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(650)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(200892416)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000022" TASK_TYPE="MAP" START_TIME="1450105162746" SPLITS="/default-rack/localhost\.localdomain" .
Task TASKID="task_201512140913_0002_m_000023" TASK_TYPE="MAP" START_TIME="1450105162746" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000022" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000022_0" START_TIME="1450105162750" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000022" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000022_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105168803" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log7\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(258823)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65110)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(670)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201490432)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000022" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105169098" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(258823)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(65110)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(670)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201490432)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000024" TASK_TYPE="MAP" START_TIME="1450105169099" SPLITS="/default-rack/localhost\.localdomain" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000023" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000023_0" START_TIME="1450105162752" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000023" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000023_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105169415" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log8\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260168)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66455)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(670)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201097216)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000023" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105169706" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260168)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66455)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(670)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(201097216)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000024" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000024_0" START_TIME="1450105169104" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="MAP" TASKID="task_201512140913_0002_m_000024" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000024_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105172593" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="hdfs://0\.0\.0\.0:8020/user/training/task3/inputFolder2/Log9\.txt:0+99000" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260101)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66388)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(680)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202067968)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000024" TASK_TYPE="MAP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105172729" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(260101)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(99114)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(1000)][(MAP_OUTPUT_RECORDS)(Map output records)(1000)][(MAP_OUTPUT_BYTES)(Map output bytes)(66388)][(SPLIT_RAW_BYTES)(Input split bytes)(114)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(1000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(680)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(202067968)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(155783168)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(99000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
ReduceAttempt TASK_TYPE="REDUCE" TASKID="task_201512140913_0002_r_000000" TASK_ATTEMPT_ID="attempt_201512140913_0002_r_000000_0" START_TIME="1450105096123" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
ReduceAttempt TASK_TYPE="REDUCE" TASKID="task_201512140913_0002_r_000000" TASK_ATTEMPT_ID="attempt_201512140913_0002_r_000000_0" TASK_STATUS="SUCCESS" SHUFFLE_FINISHED="1450105172939" SORT_FINISHED="1450105174439" FINISH_TIME="1450105176350" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="reduce > reduce" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(1700783)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(1892065)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(23197)][(HDFS_READ_OPS)(HDFS: Number of read operations)(1)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(2)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_INPUT_GROUPS)(Reduce input groups)(400)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(1700927)][(REDUCE_INPUT_RECORDS)(Reduce input records)(25000)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(400)][(SPILLED_RECORDS)(Spilled Records)(25000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(2820)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(109445120)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(728879104)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(56033280)]}{(solution\.Reduce$Counters)(solution\.Reduce$Counters)[(OUTPUT_KEYS)(OUTPUT_KEYS)(400)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_r_000000" TASK_TYPE="REDUCE" TASK_STATUS="SUCCESS" FINISH_TIME="1450105176354" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(1700783)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(1892065)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(23197)][(HDFS_READ_OPS)(HDFS: Number of read operations)(1)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(2)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_INPUT_GROUPS)(Reduce input groups)(400)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(1700927)][(REDUCE_INPUT_RECORDS)(Reduce input records)(25000)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(400)][(SPILLED_RECORDS)(Spilled Records)(25000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(2820)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(109445120)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(728879104)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(56033280)]}{(solution\.Reduce$Counters)(solution\.Reduce$Counters)[(OUTPUT_KEYS)(OUTPUT_KEYS)(400)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000025" TASK_TYPE="CLEANUP" START_TIME="1450105176356" SPLITS="" .
MapAttempt TASK_TYPE="CLEANUP" TASKID="task_201512140913_0002_m_000025" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000025_0" START_TIME="1450105176359" TRACKER_NAME="tracker_localhost\.localdomain:localhost\.localdomain/127\.0\.0\.1:37519" HTTP_PORT="50060" .
MapAttempt TASK_TYPE="CLEANUP" TASKID="task_201512140913_0002_m_000025" TASK_ATTEMPT_ID="attempt_201512140913_0002_m_000025_0" TASK_STATUS="SUCCESS" FINISH_TIME="1450105179501" HOSTNAME="/default-rack/localhost\.localdomain" STATE_STRING="cleanup" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(191675)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(2)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(140)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(96722944)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(56033280)]}nullnullnullnullnullnullnullnullnullnullnullnullnull" .
Task TASKID="task_201512140913_0002_m_000025" TASK_TYPE="CLEANUP" TASK_STATUS="SUCCESS" FINISH_TIME="1450105179684" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(191675)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(2)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(2)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(SPILLED_RECORDS)(Spilled Records)(0)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(140)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(96722944)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(721498112)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(56033280)]}nullnullnullnullnullnullnullnullnullnullnullnullnull" .
Job JOBID="job_201512140913_0002" FINISH_TIME="1450105179685" JOB_STATUS="SUCCESS" FINISHED_MAPS="25" FINISHED_REDUCES="1" FAILED_MAPS="0" FAILED_REDUCES="0" MAP_COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(0)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(6493592)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(2477865)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(0)][(HDFS_READ_OPS)(HDFS: Number of read operations)(50)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(25000)][(MAP_OUTPUT_RECORDS)(Map output records)(25000)][(MAP_OUTPUT_BYTES)(Map output bytes)(1650777)][(SPLIT_RAW_BYTES)(Input split bytes)(2865)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(SPILLED_RECORDS)(Spilled Records)(25000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(16880)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(5046095872)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(18037452800)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(3894579200)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(2475000)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" REDUCE_COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(1700783)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(1892065)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(0)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(23197)][(HDFS_READ_OPS)(HDFS: Number of read operations)(1)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(2)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_INPUT_GROUPS)(Reduce input groups)(400)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(1700927)][(REDUCE_INPUT_RECORDS)(Reduce input records)(25000)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(400)][(SPILLED_RECORDS)(Spilled Records)(25000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(2820)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(109445120)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(728879104)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(56033280)]}{(solution\.Reduce$Counters)(solution\.Reduce$Counters)[(OUTPUT_KEYS)(OUTPUT_KEYS)(400)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" COUNTERS="{(org\.apache\.hadoop\.mapreduce\.FileSystemCounter)(File System Counters)[(FILE_BYTES_READ)(FILE: Number of bytes read)(1700783)][(FILE_BYTES_WRITTEN)(FILE: Number of bytes written)(8385657)][(FILE_READ_OPS)(FILE: Number of read operations)(0)][(FILE_LARGE_READ_OPS)(FILE: Number of large read operations)(0)][(FILE_WRITE_OPS)(FILE: Number of write operations)(0)][(HDFS_BYTES_READ)(HDFS: Number of bytes read)(2477865)][(HDFS_BYTES_WRITTEN)(HDFS: Number of bytes written)(23197)][(HDFS_READ_OPS)(HDFS: Number of read operations)(51)][(HDFS_LARGE_READ_OPS)(HDFS: Number of large read operations)(0)][(HDFS_WRITE_OPS)(HDFS: Number of write operations)(2)]}{(org\.apache\.hadoop\.mapreduce\.JobCounter)(Job Counters )[(TOTAL_LAUNCHED_MAPS)(Launched map tasks)(25)][(TOTAL_LAUNCHED_REDUCES)(Launched reduce tasks)(1)][(DATA_LOCAL_MAPS)(Data-local map tasks)(25)][(SLOTS_MILLIS_MAPS)(Total time spent by all maps in occupied slots \\(ms\\))(165155)][(SLOTS_MILLIS_REDUCES)(Total time spent by all reduces in occupied slots \\(ms\\))(80227)][(FALLOW_SLOTS_MILLIS_MAPS)(Total time spent by all maps waiting after reserving slots \\(ms\\))(0)][(FALLOW_SLOTS_MILLIS_REDUCES)(Total time spent by all reduces waiting after reserving slots \\(ms\\))(0)]}{(org\.apache\.hadoop\.mapreduce\.TaskCounter)(Map-Reduce Framework)[(MAP_INPUT_RECORDS)(Map input records)(25000)][(MAP_OUTPUT_RECORDS)(Map output records)(25000)][(MAP_OUTPUT_BYTES)(Map output bytes)(1650777)][(SPLIT_RAW_BYTES)(Input split bytes)(2865)][(COMBINE_INPUT_RECORDS)(Combine input records)(0)][(COMBINE_OUTPUT_RECORDS)(Combine output records)(0)][(REDUCE_INPUT_GROUPS)(Reduce input groups)(400)][(REDUCE_SHUFFLE_BYTES)(Reduce shuffle bytes)(1700927)][(REDUCE_INPUT_RECORDS)(Reduce input records)(25000)][(REDUCE_OUTPUT_RECORDS)(Reduce output records)(400)][(SPILLED_RECORDS)(Spilled Records)(50000)][(CPU_MILLISECONDS)(CPU time spent \\(ms\\))(19700)][(PHYSICAL_MEMORY_BYTES)(Physical memory \\(bytes\\) snapshot)(5155540992)][(VIRTUAL_MEMORY_BYTES)(Virtual memory \\(bytes\\) snapshot)(18766331904)][(COMMITTED_HEAP_BYTES)(Total committed heap usage \\(bytes\\))(3950612480)]}{(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)(org\.apache\.hadoop\.mapreduce\.lib\.input\.FileInputFormatCounter)[(BYTES_READ)(BYTES_READ)(2475000)]}{(solution\.Reduce$Counters)(solution\.Reduce$Counters)[(OUTPUT_KEYS)(OUTPUT_KEYS)(400)]}nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull" .
